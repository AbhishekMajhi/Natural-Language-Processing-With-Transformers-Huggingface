{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45a7a089-5ade-4bb7-b600-46609e60adbc",
   "metadata": {},
   "source": [
    "# Install"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "512eebfb-c23b-4a71-88a2-d0312dcb0e86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install torchdata\n",
    "# %pip install datasets\n",
    "# %pip install evaluate\n",
    "# %pip install rouge_score\n",
    "# %pip install loralib\n",
    "# %pip install peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f98353f-ba32-46b3-b24d-9d2f9efc3ea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# os.environ(TOKENIZERS_PARALLELISM=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bd99065-f6df-41ec-83fb-b097e351b347",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ec450b4f-4e1b-4cff-a8e0-faee109a53c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer, GenerationConfig, TrainingArguments, Trainer\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import evaluate"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3643af06-6a05-4e76-99ee-0aecbd490f8a",
   "metadata": {},
   "source": [
    "# Summarize Dialogue data without prompt engineering\n",
    "\n",
    "Here we will be generating a summary of a dialogue with LLM (FLAN-T5) from HuggingFace.\n",
    "Here we gonna use DialogSum dataset from HuggingFace dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e99d1c03-36c1-4c9a-bf18-d4825745d7b6",
   "metadata": {},
   "source": [
    "## Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4743579f-06fa-4a66-89fc-e161b38835d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_name = \"knkarthick/dialogsum\"\n",
    "dataset = load_dataset(dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8efaff23-e041-44f2-8142-ed732c698f02",
   "metadata": {},
   "source": [
    "Print some dialogue with their base line summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e37b041-c702-4e0d-972a-458d765ede01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "Example:  1\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT_DIALOGUE: \n",
      "#Person1#: What time is it, Tom?\n",
      "#Person2#: Just a minute. It's ten to nine by my watch.\n",
      "#Person1#: Is it? I had no idea it was so late. I must be off now.\n",
      "#Person2#: What's the hurry?\n",
      "#Person1#: I must catch the nine-thirty train.\n",
      "#Person2#: You've plenty of time yet. The railway station is very close. It won't take more than twenty minutes to get there.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY: \n",
      "#Person1# is in a hurry to catch a train. Tom tells #Person1# there is plenty of time.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "Example:  2\n",
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT_DIALOGUE: \n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY: \n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n"
     ]
    }
   ],
   "source": [
    "exa_indices = [40, 200]\n",
    "dash_line = '-'.join('' for x in range(100))\n",
    "\n",
    "for i, index in enumerate(exa_indices):\n",
    "    print(dash_line)\n",
    "    print(\"Example: \", i + 1)\n",
    "    print(dash_line)\n",
    "    print(\"INPUT_DIALOGUE: \")\n",
    "    print(dataset['test'][index]['dialogue'])\n",
    "    print(dash_line)\n",
    "    print(\"BASELINE HUMAN SUMMARY: \")\n",
    "    print(dataset['test'][index]['summary'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7c8b3c4-d77e-44dc-9274-2acef71b89c4",
   "metadata": {},
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "91d631aa-805d-41c9-a109-ed19f816062c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"google/flan-t5-base\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5897b566-a946-49d9-b2ef-ed8ed3a799bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ENCODED SENTENCE\n",
      "tensor([  27,   43,  373, 6528,   25,   55,    1])\n",
      "\n",
      "SENTENCE DECODED\n",
      "I have always liked you!\n"
     ]
    }
   ],
   "source": [
    "# testing tokenizer\n",
    "sentence = \" I have always liked you!\"\n",
    "sentence_encoded = tokenizer(sentence, return_tensors='pt')\n",
    "sentence_decoded = tokenizer.decode(sentence_encoded['input_ids'][0], skip_special_tokens=True)\n",
    "print(\"ENCODED SENTENCE\")\n",
    "print(sentence_encoded['input_ids'][0])\n",
    "print(\"\\nSENTENCE DECODED\")\n",
    "print(sentence_decoded)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a686766-e94d-4d11-8b12-76b78f257834",
   "metadata": {},
   "source": [
    "## Util Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a7be3ed0-f2c7-404c-b0bb-3ada8eb345d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 247577856\n",
      "all model parameters: 247577856\n",
      "percentage of trainable mdoel parameter: 100.00%\n"
     ]
    }
   ],
   "source": [
    "def print_models_trainable_parameters(model):\n",
    "    trainable_param = 0\n",
    "    all_model_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_model_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_param += param.numel()\n",
    "    return f\"trainable model parameters: {trainable_param}\\nall model parameters: {all_model_param}\\npercentage of trainable mdoel parameter: {100*trainable_param/all_model_param:.2f}%\"\n",
    "print(print_models_trainable_parameters(model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4533a71c-079a-4c39-b6ee-58bb058cc40c",
   "metadata": {},
   "source": [
    "From this we can see that here we have to train all the parameters. This is a full fine-tuning.<br>\n",
    "Here we gonna do full fine-tuning and PEFT with LORa.<br>\n",
    "Before moving into Full Fine-Tuning lets do a zero short first to test our FLAN-T5 performance before doing any\n",
    "sort of fine-tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0993cae5-3e70-4fff-ac6f-6b955f99fcfa",
   "metadata": {},
   "source": [
    "## Zero Short In-Context Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b072ddf6-509f-49d4-b185-bc28698d7876",
   "metadata": {},
   "outputs": [],
   "source": [
    "index = 200\n",
    "dialogue = dataset[\"test\"][index][\"dialogue\"]\n",
    "summary = dataset[\"test\"][index][\"summary\"]\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "{dialogue}\n",
    "\n",
    "Summary: \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77243a33-c5a6-4b96-bdc9-3d08c4a6329f",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "outputs = tokenizer.decode(\n",
    "    model.generate(inputs[\"input_ids\"], max_new_tokens=200,)[0],\n",
    "    skip_special_tokens=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a8740e67-f96a-4dd6-9900-18ee712e1f21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "INPUT PROMPT:\n",
      "\n",
      "Summarize the following conversation.\n",
      "#Person1#: Have you considered upgrading your system?\n",
      "#Person2#: Yes, but I'm not sure what exactly I would need.\n",
      "#Person1#: You could consider adding a painting program to your software. It would allow you to make up your own flyers and banners for advertising.\n",
      "#Person2#: That would be a definite bonus.\n",
      "#Person1#: You might also want to upgrade your hardware because it is pretty outdated now.\n",
      "#Person2#: How can we do that?\n",
      "#Person1#: You'd probably need a faster processor, to begin with. And you also need a more powerful hard disc, more memory and a faster modem. Do you have a CD-ROM drive?\n",
      "#Person2#: No.\n",
      "#Person1#: Then you might want to add a CD-ROM drive too, because most new software programs are coming out on Cds.\n",
      "#Person2#: That sounds great. Thanks.\n",
      "\n",
      "Summary: \n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "\n",
      "---------------------------------------------------------------------------------------------------\n",
      "MODEL GENERATION - ZERO SHOT:\n",
      "#Person1#: I'm thinking of upgrading my computer.\n"
     ]
    }
   ],
   "source": [
    "dash_line = '-'.join('' for x in range(100))\n",
    "print(dash_line)\n",
    "print(f'INPUT PROMPT:\\n{prompt}')\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{summary}\\n')\n",
    "print(dash_line)\n",
    "print(f'MODEL GENERATION - ZERO SHOT:\\n{outputs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efb9a1b1-e816-4f81-9b63-eef5ec6ba468",
   "metadata": {},
   "source": [
    "As you can see its not able to give satisfied results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28aab909-4c4f-45ec-9168-9efaa7f9d533",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f53d7036-6f58-4c3b-99aa-4fbc11b02306",
   "metadata": {},
   "source": [
    "Here we need to do add a instruction to the start of the dialogue for LLM to understand.\n",
    "Example\n",
    "\n",
    "Summarize the following conversation.\n",
    "\n",
    "    Chris: This is his part of the conversation.\n",
    "    Antje: This is her part of the conversation.\n",
    "\n",
    "Summary:\n",
    "Both Chris and Antje participated in the conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2e38e9b8-31d2-42af-881d-d0fcc42bf4a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3164911a559e4be2921d5b7a2c12eee4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def tokenize_fun(example):\n",
    "    start_prompt = \"Summarize the following conversation.\\n\\n\"\n",
    "    end_prompt = \"\\n\\nSummary: \"\n",
    "    prompt = [start_prompt + dialogue + end_prompt for dialogue in example[\"dialogue\"]]\n",
    "    example[\"input_ids\"] = tokenizer(prompt, padding= \"max_length\",truncation=True,return_tensors='pt').input_ids\n",
    "    example[\"labels\"] = tokenizer(example[\"summary\"], padding='max_length', truncation=True, return_tensors=\"pt\").input_ids\n",
    "    return example\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_fun, batched=True)\n",
    "tokenized_dataset = tokenized_dataset.remove_columns([\"id\", \"topic\", \"dialogue\", \"summary\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e7d0ff87-2325-4d08-8866-b9a86f1f5f90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of the dataset\n",
      "Training: (12460, 2)\n",
      "Validation: (500, 2)\n",
      "Test: (1500, 2)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of the dataset\")\n",
    "print(f\"Training: {tokenized_dataset['train'].shape}\")\n",
    "print(f\"Validation: {tokenized_dataset['validation'].shape}\")\n",
    "print(f\"Test: {tokenized_dataset['test'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "473117bb-be9a-4597-b9a5-63048d775894",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 12460\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 500\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['input_ids', 'labels'],\n",
      "        num_rows: 1500\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a27921df-5006-4f15-9073-2fb5a0f3a13b",
   "metadata": {},
   "source": [
    "## PEFT ith LoRA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b31c3396-d2c3-4d34-821a-84623df3d730",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model, TaskType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dc187dec-740d-4f3e-b448-68c4cd9a31ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig(\n",
    "    r=32, # rank\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"q\", \"v\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias = \"none\",\n",
    "    task_type=TaskType.SEQ_2_SEQ_LM # FLAN-T5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5f6c94d9-87a2-4d58-8369-5b34b4ad108b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable model parameters: 3538944\n",
      "all model parameters: 251116800\n",
      "percentage of trainable mdoel parameter: 1.41%\n"
     ]
    }
   ],
   "source": [
    "peft_model = get_peft_model(model,lora_config)\n",
    "print(print_models_trainable_parameters(peft_model))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3cbc9c-5d62-4ece-98f9-29259937fd7b",
   "metadata": {},
   "source": [
    "#### Train PEFT Adapter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d7236d84-68c1-4e2a-9306-a42f2e991bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "output_dir = f\"./peft-dialogue-summary-training-{str(int(time.time()))}\"\n",
    "\n",
    "peft_training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    auto_find_batch_size=True,\n",
    "    learning_rate=1e-5, # It's higher learning rate than full fine-tuning.\n",
    "    num_train_epochs=5,\n",
    "    logging_steps=1,\n",
    "    max_steps=1\n",
    ")\n",
    "\n",
    "peft_trainer = Trainer(\n",
    "    model=peft_model,\n",
    "    args=peft_training_args,\n",
    "    train_dataset=tokenized_dataset[\"train\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "bae0e33c-2f7b-43e3-8a3b-095fb3d966dc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1/1 00:00, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>34.729500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1, training_loss=34.729515075683594, metrics={'train_runtime': 4.0867, 'train_samples_per_second': 0.979, 'train_steps_per_second': 0.245, 'total_flos': 2782515953664.0, 'train_loss': 34.729515075683594, 'epoch': 0.00032102728731942215})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()\n",
    "peft_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1203f804-a716-4e1f-9419-5a89f3db446a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------------------------------------------------------------------\n",
      "BASELINE HUMAN SUMMARY:\n",
      "#Person1# teaches #Person2# how to upgrade software and hardware in #Person2#'s system.\n",
      "---------------------------------------------------------------------------------------------------\n",
      "PEFT MODEL: #Person1: I'm not sure what you're looking for. #Person2: I'm not sure what exactly I'm looking for. #Person1: I'm not sure what you're talking about. #Person2: I'm not sure what exactly you're talking about. #Person1: I'm not sure what you're talking about. #Person2: I'm not sure what you're talking about. #Person1: I'm not sure what you're talking about. #Person2: I'm not sure what you're talking about. #Person1: I'm not sure what you're talking about. #Person1: I'm not sure what you're talking about. #Person2: I'm not sure what you're talking about.\n"
     ]
    }
   ],
   "source": [
    "index = 200\n",
    "dialogue = dataset['test'][index]['dialogue']\n",
    "baseline_human_summary = dataset['test'][index]['summary']\n",
    "\n",
    "prompt = f\"\"\"\n",
    "Summarize the following conversation.\n",
    "\n",
    "{dialogue}\n",
    "\n",
    "Summary: \"\"\"\n",
    "\n",
    "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "input_ids = input_ids.to(device)\n",
    "peft_model = peft_model.to(device)\n",
    "peft_model_outputs = peft_model.generate(\n",
    "    input_ids=input_ids,\n",
    "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n",
    ")\n",
    "# instruct_model_outputs = instruct_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "# instruct_model_text_output = tokenizer.decode(instruct_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "# peft_model_outputs = peft_model.generate(input_ids=input_ids, generation_config=GenerationConfig(max_new_tokens=200, num_beams=1))\n",
    "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
    "\n",
    "print(dash_line)\n",
    "print(f'BASELINE HUMAN SUMMARY:\\n{baseline_human_summary}')\n",
    "# print(dash_line)\n",
    "# print(f'ORIGINAL MODEL:\\n{original_model_text_output}')\n",
    "# print(dash_line)\n",
    "# print(f'INSTRUCT MODEL:\\n{instruct_model_text_output}')\n",
    "print(dash_line)\n",
    "print(f'PEFT MODEL: {peft_model_text_output}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b44068fb-e426-4aae-b852-50cc6cb135ae",
   "metadata": {},
   "source": [
    "## Full Fine-Tuning Process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8d0262ec-c074-435a-93d5-d70e2c69a53a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "max_steps is given, it will override any value given in num_train_epochs\n"
     ]
    }
   ],
   "source": [
    "output_dir = f'./dialogue-summary-training-{str(int(time.time()))}'\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=output_dir,\n",
    "    learning_rate=1e-5,\n",
    "    num_train_epochs=2,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=1,\n",
    "    max_steps=1,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset['train'],\n",
    "    eval_dataset=tokenized_dataset[\"validation\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9041469a-9210-4263-802f-59520c33d509",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear GPU cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "beac4c49-118e-4ed2-a445-af1f32985cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'|===========================================================================|\\n|                  PyTorch CUDA memory summary, device ID 0                 |\\n|---------------------------------------------------------------------------|\\n|            CUDA OOMs: 2            |        cudaMalloc retries: 2         |\\n|===========================================================================|\\n|        Metric         | Cur Usage  | Peak Usage | Tot Alloc  | Tot Freed  |\\n|---------------------------------------------------------------------------|\\n| Allocated memory      |  14844 MiB |  14844 MiB |  23173 MiB |   8328 MiB |\\n|       from large pool |  14843 MiB |  14843 MiB |  23157 MiB |   8314 MiB |\\n|       from small pool |      1 MiB |      3 MiB |     15 MiB |     14 MiB |\\n|---------------------------------------------------------------------------|\\n| Active memory         |  14844 MiB |  14844 MiB |  23173 MiB |   8328 MiB |\\n|       from large pool |  14843 MiB |  14843 MiB |  23157 MiB |   8314 MiB |\\n|       from small pool |      1 MiB |      3 MiB |     15 MiB |     14 MiB |\\n|---------------------------------------------------------------------------|\\n| Requested memory      |  14832 MiB |  14832 MiB |  23161 MiB |   8328 MiB |\\n|       from large pool |  14831 MiB |  14831 MiB |  23145 MiB |   8314 MiB |\\n|       from small pool |      1 MiB |      3 MiB |     15 MiB |     14 MiB |\\n|---------------------------------------------------------------------------|\\n| GPU reserved memory   |  14938 MiB |  14938 MiB |  14940 MiB |   2048 KiB |\\n|       from large pool |  14936 MiB |  14936 MiB |  14936 MiB |      0 KiB |\\n|       from small pool |      2 MiB |      4 MiB |      4 MiB |   2048 KiB |\\n|---------------------------------------------------------------------------|\\n| Non-releasable memory |  95802 KiB | 194350 KiB |   5175 MiB |   5081 MiB |\\n|       from large pool |  94976 KiB | 193280 KiB |   5158 MiB |   5065 MiB |\\n|       from small pool |    826 KiB |   2506 KiB |     16 MiB |     15 MiB |\\n|---------------------------------------------------------------------------|\\n| Allocations           |     914    |     914    |    1415    |     501    |\\n|       from large pool |     788    |     788    |    1150    |     362    |\\n|       from small pool |     126    |     128    |     265    |     139    |\\n|---------------------------------------------------------------------------|\\n| Active allocs         |     914    |     914    |    1415    |     501    |\\n|       from large pool |     788    |     788    |    1150    |     362    |\\n|       from small pool |     126    |     128    |     265    |     139    |\\n|---------------------------------------------------------------------------|\\n| GPU reserved segments |     439    |     439    |     440    |       1    |\\n|       from large pool |     438    |     438    |     438    |       0    |\\n|       from small pool |       1    |       2    |       2    |       1    |\\n|---------------------------------------------------------------------------|\\n| Non-releasable allocs |      55    |      56    |     321    |     266    |\\n|       from large pool |      51    |      53    |     208    |     157    |\\n|       from small pool |       4    |       4    |     113    |     109    |\\n|---------------------------------------------------------------------------|\\n| Oversize allocations  |       0    |       0    |       0    |       0    |\\n|---------------------------------------------------------------------------|\\n| Oversize GPU segments |       0    |       0    |       0    |       0    |\\n|===========================================================================|\\n'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.memory_summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7531011b-ec95-4a4f-bb63-b0185991bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
