{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7c602a00-6d05-4550-8408-74499e521d72",
   "metadata": {},
   "source": [
    "# Install these packages if not installed\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2120e32c-e10d-49ad-983a-158d7e871db9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install langchain\n",
    "# !pip install chromadb\n",
    "# !pip install pypdf\n",
    "# !pip install pytest\n",
    "# !pip install accelerate\n",
    "# !pip install -U bitsandbytes\n",
    "# %pip install -qU langchain-google-vertexai\n",
    "#!pip install sacremoses\n",
    "#!pip install -qU langchain-community faiss-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d791183-28cb-46fe-b2f7-9e0bfd7dd760",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# All Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "853ab53b-5d3e-46de-9f6f-1b0853be5e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "# for Google Cloud AutoML\n",
    "from google.oauth2 import service_account\n",
    "import vertexai\n",
    "import json\n",
    "\n",
    "# LangChain imports\n",
    "from langchain.schema.document import Document\n",
    "from langchain_community.document_loaders import PyPDFDirectoryLoader\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_google_vertexai import VertexAIEmbeddings\n",
    "\n",
    "# Vector DB\n",
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain_community.docstore.in_memory import InMemoryDocstore\n",
    "import faiss\n",
    "from uuid import uuid4\n",
    "\n",
    "# Template\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "# Huggingface Login\n",
    "from huggingface_hub import login\n",
    "\n",
    "# LLM\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from transformers import pipeline\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain.chains import LLMChain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf0b527-64ab-4308-88d9-69034f1f0441",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Google Cloud AutoML Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "242626d2-aaa7-444d-86f3-2dfd968adb32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_google_cloud_autoML():\n",
    "    # load the JSON file\n",
    "    with open('./data/probable-life-441114-n1-d2f8fa3aef61.json') as source:\n",
    "        info = json.load(source)\n",
    "    \n",
    "    vertex_cred = service_account.Credentials.from_service_account_info(info)\n",
    "    \n",
    "    PROJECT_ID = \"probable-life-441114-n1\"\n",
    "    REGION = \"asia-south1\"\n",
    "    vertexai.init(project=PROJECT_ID,\n",
    "                 location=REGION,\n",
    "                 credentials=vertex_cred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86548d2d-0419-4918-987e-1580b6eb00f0",
   "metadata": {},
   "source": [
    "# Data loading fucnction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "58490e2e-d556-40e5-ba82-1847a8ed3030",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_process_data(DATA_DIRECTORY):\n",
    "    loader = PyPDFDirectoryLoader(DATA_DIRECTORY)\n",
    "    data =  loader.load()\n",
    "    spliter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size = 500,\n",
    "        chunk_overlap = 100,\n",
    "        length_function = len,\n",
    "        is_separator_regex = False, # use if your separators are plain text and not regex patterns.\n",
    "    )\n",
    "    return spliter.split_documents(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebac285a-37f4-46c2-8e7b-d83830ae0dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chunks = load_and_process_data(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079f36d6-f693-451d-afbe-188347fdd157",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Vector DB FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "b231b5e4-794e-4c4c-acca-b4edb9d2801f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_vector_db():\n",
    "    # get the indexing with max embedding lenght\n",
    "    index = faiss.IndexFlatL2(len(embeddings.embed_query(\"hello world\")))\n",
    "    # initialize the DB\n",
    "    vector_db = FAISS(\n",
    "        embedding_function= embeddings,\n",
    "        index = index,\n",
    "        docstore = InMemoryDocstore(),\n",
    "        index_to_docstore_id = {}\n",
    "    )\n",
    "    return vector_db\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460dc4ec-5750-4d3e-9979-c19ec17ff1b9",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "a3d789b6-e929-46f8-86a9-a7e537b9705f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similarities(query_text):\n",
    "    result = vector_db.similarity_search(query = query_text)\n",
    "    context = \"\\n\".join([doc.page_content for doc in result])\n",
    "    return result, context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "15e8655c-557f-4ecf-bbe4-6db20a137371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#res = get_similarities(query_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49d9ac35-994e-4e4a-b91a-1d3ed0d61a43",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Set Huggingface token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "05013ac7-3f05-4446-a387-9bec52c4744a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "28dce80ed47044649f519e062624037d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 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\n",
    "login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1248c3-8c7d-4d45-b170-abc0a436f706",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Load the LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "baa403a0-215e-46df-9399-2a1145088656",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_llm():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\"microsoft/biogpt\")\n",
    "    # model = AutoModelForCausalLM.from_pretrained(\"microsoft/biogpt\")\n",
    "    from transformers import BioGptForCausalLM\n",
    "    model = BioGptForCausalLM.from_pretrained(\"microsoft/biogpt\", \n",
    "                                              attn_implementation=\"sdpa\", \n",
    "                                              torch_dtype=torch.float16,\n",
    "                                             )\n",
    "    return tokenizer, model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fc4ef60-1415-452a-95c3-47f40e344326",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Generate Response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a03860ef-ee76-420f-be77-13aaaf5f5dd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_response():\n",
    "    response = chain.invoke({\"context\": context, \"query_text\": query_text})\n",
    "    matches = re.search(r\"Question:\\s*(.*?)\\n\\nAnswer:\\s*(.*)\",response.get(\"text\"), re.DOTALL)\n",
    "    if matches:\n",
    "        question = matches.group(1).strip()\n",
    "        answer = matches.group(2).strip()\n",
    "        print(f\"Question: {question}\")\n",
    "        print(f\"Answer: {answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289ee7f-8982-4aad-947e-ee990c50f7ec",
   "metadata": {},
   "source": [
    "# Necessary Function calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "dc2a767c-b289-4c08-8416-58636babf1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Question goes here for now\n",
    "query_text = \"How does pulmonary emphysema affect electrocardiographic potentials?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "a1f0a449-4b16-4bf2-8789-1c18ff4a4c35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to setup GCAML\n",
    "set_google_cloud_autoML()\n",
    "# Load and Process data\n",
    "chunks = load_and_process_data(\"data\")  # data is the directory name\n",
    "# Initialize the a specific Embeddings Model version\n",
    "embeddings = VertexAIEmbeddings(model_name=\"textembedding-gecko@003\")\n",
    "# Add data into vector DB\n",
    "vector_db = initialize_vector_db()\n",
    "uuids = [str(uuid4()) for _ in range(len(chunks))]\n",
    "vector_db.add_documents(documents= chunks, ids = uuids)\n",
    "\n",
    "# define reteiever\n",
    "retriever = vector_db.as_retriever()\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "You are a biomedical expert. Based on the information provided below, answer the question concisely.\n",
    "\n",
    "Information: {context}\n",
    "\n",
    "Question: {query_text}\n",
    "\n",
    "Answer:\n",
    "\"\"\"\n",
    "prompt_template = PromptTemplate(\n",
    "    input_variables=[\"context\", \"query_text\"],\n",
    "    template=PROMPT_TEMPLATE\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "0da9496a-891e-407b-b445-99bd30bbd750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve\n",
    "result, context = get_similarities(query_text=query_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f669ba4a-d92e-497c-96dd-2866a78d4eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7016/2616295316.py:5: LangChainDeprecationWarning: The class `HuggingFacePipeline` was deprecated in LangChain 0.0.37 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFacePipeline``.\n",
      "  llm = HuggingFacePipeline(pipeline=pipe)\n",
      "/tmp/ipykernel_7016/2616295316.py:7: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
      "  chain = LLMChain(prompt=prompt_template, llm=llm)\n"
     ]
    }
   ],
   "source": [
    "# Pipeline\n",
    "tokenizer, model = load_llm()\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=500, device=0)\n",
    "# Create a LangChain LLM instance\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "# Chain Old School\n",
    "chain = LLMChain(prompt=prompt_template, llm=llm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "17d3c79c-664d-4455-b931-ff248cd099aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: How does pulmonary emphysema affect electrocardiographic potentials?\n",
      "Answer: Pulmonary emphysema can decrease the electro- cardiographic potentials, but for a different reason than that of pericardial effusion.\n"
     ]
    }
   ],
   "source": [
    "get_response()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3002d2fb-8c40-4fda-affb-f9c507050e33",
   "metadata": {},
   "source": [
    "# Next design a Chat like system with chat follow up facility."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
